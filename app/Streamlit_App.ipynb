{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOmAqPUzdkffehNGLGNVECY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChaithanyaSaiB/UMBC-DATA606-Capstone/blob/main/app/Streamlit_App.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "r1taDF6Fv__T"
      },
      "outputs": [],
      "source": [
        "!pip install -q streamlit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5j-qtzXPxGGL",
        "outputId": "0fac4ae9-a3d7-43a5-b918-ae92891f4b40"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[34;40mnotice\u001b[0m\u001b[35m\u001b[0m created a lockfile as package-lock.json. You should commit this file.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "+ localtunnel@2.0.2\n",
            "added 22 packages from 22 contributors and audited 22 packages in 1.756s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 1 \u001b[93mmoderate\u001b[0m severity vulnerability\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n",
            "\u001b[K\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile prediction.py\n",
        "\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('words')\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "\n",
        "# Function to fetch the content from a URL\n",
        "def fetch_content(url):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        if response.status_code == 200:\n",
        "            return response.text\n",
        "        else:\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(\"Error fetching content:\", e)\n",
        "        return None\n",
        "\n",
        "# Function to extract body content from HTML using BeautifulSoup\n",
        "def extract_body(html_content):\n",
        "    if html_content:\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "        article_body = soup.findAll('p', class_='paragraph')\n",
        "        if article_body:\n",
        "            return article_body\n",
        "        else:\n",
        "            return None\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def split_string_with_special_characters(word_list):\n",
        "  # Define pattern for special characters\n",
        "  pattern = r'[-\\s]'\n",
        "\n",
        "  for word in word_list:\n",
        "    if re.search(r'[-\\s]', word):\n",
        "      # Split the string using the pattern\n",
        "      substrings = re.split(pattern, word)\n",
        "\n",
        "      # Remove empty substrings\n",
        "      substrings = [substr for substr in substrings if substr]\n",
        "\n",
        "      word_list.remove(word)\n",
        "      word_list.extend(substrings)\n",
        "  return word_list\n",
        "\n",
        "def lowercase_words_and_lemmatize(word_list):\n",
        "    # Convert words to lowercase\n",
        "    word_list_lower = [word.lower() for word in word_list]\n",
        "\n",
        "    # Tag POS for each word\n",
        "    pos_tags = nltk.pos_tag(word_list_lower)\n",
        "\n",
        "    # Map POS tags to WordNet POS tags\n",
        "    def get_wordnet_pos(tag):\n",
        "        if tag.startswith('J'):\n",
        "            return wordnet.ADJ\n",
        "        elif tag.startswith('V'):\n",
        "            return wordnet.VERB\n",
        "        elif tag.startswith('N'):\n",
        "            return wordnet.NOUN\n",
        "        elif tag.startswith('R'):\n",
        "            return wordnet.ADV\n",
        "        else:\n",
        "            return wordnet.NOUN  # Default to noun if POS tag not found\n",
        "\n",
        "    # Lemmatize words using correct POS tags\n",
        "    lemmatized_words = []\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    for word, tag in pos_tags:\n",
        "        pos = get_wordnet_pos(tag)\n",
        "        lemma = lemmatizer.lemmatize(word, pos=pos)\n",
        "        lemmatized_words.append(lemma)\n",
        "\n",
        "    return lemmatized_words\n",
        "\n",
        "parts_of_speech_to_remove = ['DT', 'IN', 'PRP', 'PRP$', 'CC', 'VB', 'JJ']\n",
        "\n",
        "def stopwords_removal(word_list):\n",
        "  # Tag words with their parts of speech\n",
        "  tagged_words = nltk.pos_tag(word_list)\n",
        "\n",
        "  # Get a list of common English stopwords\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "\n",
        "  f = open('custom stopwords.txt','r')\n",
        "  custom_stopwords = [word.strip() for word in f.readlines()]\n",
        "  f.close()\n",
        "\n",
        "  return [word for word, pos in tagged_words if pos not in parts_of_speech_to_remove and word not in stop_words and word not in custom_stopwords]\n",
        "\n",
        "def remove_non_alphabetic_and_custom_stopwords(word_list):\n",
        "    english_words = set(nltk.corpus.words.words())\n",
        "\n",
        "    # Remove non-alphabetic characters and filter out empty strings\n",
        "    return [word for word in [re.sub(r'[^a-zA-Z]', '', word) for word in word_list] if word in english_words]\n",
        "\n",
        "index_to_topic = {\n",
        "    0: \"Vaccination and Measles\",\n",
        "    1: \"Medical Treatments and Marijuana\",\n",
        "    2: \"Health Research and Findings\",\n",
        "    3: \"Childhood Mental Health and Education\",\n",
        "    4: \"Infectious Diseases and Hygiene\",\n",
        "    5: \"Food and Nutrition\",\n",
        "    6: \"Family Health and Illness\",\n",
        "    7: \"Personal Health and Weight Management\",\n",
        "    8: \"Healthcare System and Cases\"\n",
        "}\n",
        "\n",
        "def predict(url):\n",
        "  raw_data = extract_body(fetch_content(url))\n",
        "  content = \" \".join([p_tag.text.strip() for p_tag in raw_data])\n",
        "  tokenized_data = word_tokenize(content)\n",
        "  lemmatized_data = lowercase_words_and_lemmatize(tokenized_data)\n",
        "  filtered_data = remove_non_alphabetic_and_custom_stopwords(stopwords_removal(lemmatized_data))\n",
        "\n",
        "  model_dictionary = Dictionary.load('dictionary.sav')\n",
        "  transformed_data = model_dictionary.doc2bow(filtered_data)\n",
        "\n",
        "  lda_model = joblib.load(\"lda_model.sav\")\n",
        "  topics_probability = lda_model.get_document_topics(transformed_data)\n",
        "  topic_number = max(topics_probability, key=lambda x: x[1])[0]\n",
        "  return index_to_topic.get(topic_number)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJDAeWPF1WGn",
        "outputId": "ed44e3e9-e63c-4eec-9ab2-7b9805bae257"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting prediction.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "from prediction import predict\n",
        "import streamlit as st\n",
        "\n",
        "st.title('Classifying CNN Health News Articles from 2015')\n",
        "st.markdown('This is a topic modelling Natural Language Processsing model that would try to extract the topic of article from the URL passed in the text underneath')\n",
        "\n",
        "st.header(\"Give a news article link to know the topic it is related to\")\n",
        "article_url = st.text_input('Article URL', '')\n",
        "\n",
        "if st.button(\"Find Topic\"):\n",
        "  result = predict(article_url)\n",
        "  st.text(\"Topic seems to be most related to \"+result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0SVfYz0xDNF",
        "outputId": "dee88003-4185-44e8-f2b7-92ef23992275"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWcIP5xCxJBA",
        "outputId": "35992266-b2d5-4f4e-c7be-24ef3b65e79f"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.80.80.8\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 3.96s\n",
            "your url is: https://smooth-doors-slide.loca.lt\n"
          ]
        }
      ]
    }
  ]
}