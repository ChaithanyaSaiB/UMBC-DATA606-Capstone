{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOfJhRhS3N4/DXPorTPP6hs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChaithanyaSaiB/UMBC-DATA606-Capstone/blob/main/notebooks/Dataset_Creation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Creation"
      ],
      "metadata": {
        "id": "clzLRbu2sNmb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, I would be taking the initial dataset from UCI Machine Learning Repository that contains health news tweets from various health agencies with URLs. An article's content is extracted using that URL. Then content is preprocessing and stored for topic modeling"
      ],
      "metadata": {
        "id": "7As_jYUusRlX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installs and imports"
      ],
      "metadata": {
        "id": "eZrs4DRLFGeH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section includes all the packages installs and imports which are necessary for execution of the notebook"
      ],
      "metadata": {
        "id": "NhrfmVCvFSQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fake-useragent"
      ],
      "metadata": {
        "id": "zN2VKf--XtCc",
        "outputId": "65da519f-062d-475c-ef2f-00d888ad9d08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fake-useragent\n",
            "  Downloading fake_useragent-1.5.1-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: fake-useragent\n",
            "Successfully installed fake-useragent-1.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import re\n",
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from urllib.parse import urlparse\n",
        "from fake_useragent import UserAgent\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Import the tqdm library for progress bar\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "8kh4t6C6WttG",
        "outputId": "92ce82fd-b27d-48f7-d8b4-c86bdde8b5c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Data and Article Content Extraction"
      ],
      "metadata": {
        "id": "2OvgUHZyF0oI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load and Combine Data Files"
      ],
      "metadata": {
        "id": "sy_y-9G0NEcV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we load and combine all the data files for 5 news agencies namely - **NPR Health**, **CNN Health**, **CBC Health**, **Everyday Health** and **LA Times Health**"
      ],
      "metadata": {
        "id": "RRLQollhGDBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the directory path containing the .txt files\n",
        "directory = '/content'\n",
        "\n",
        "# Initialize an empty list to store DataFrames\n",
        "dfs = []\n",
        "\n",
        "# Iterate over all files in the directory\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.endswith('.txt'):\n",
        "        # Read the .txt file into a DataFrame\n",
        "        df_name = os.path.splitext(filename)[0]  # Extract filename without extension\n",
        "        try:\n",
        "            df = pd.read_csv(os.path.join(directory, filename),\n",
        "                             sep=r\"(?<!\\s)[|](?!\\s)\",\n",
        "                             names=['ID', 'DateTime', 'Tweet'],\n",
        "                             engine=\"python\")\n",
        "            # Add the DataFrame to the list\n",
        "            dfs.append(df)\n",
        "            print(f\"DataFrame '{df_name}' loaded with {len(df)} rows.\")\n",
        "        except UnicodeDecodeError:\n",
        "            print(f\"Error reading {filename}: UnicodeDecodeError. Skipping this file.\")\n",
        "\n",
        "# Concatenate all DataFrames in the list into one DataFrame\n",
        "combined_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# Display the combined DataFrame\n",
        "print(\"Combined DataFrame:\")\n",
        "print(combined_df)"
      ],
      "metadata": {
        "id": "HjgVm5rcqaAd",
        "outputId": "35bf28a5-49b3-42f9-92e6-28c0e6c0161c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame 'cnnhealth' loaded with 4061 rows.\n",
            "DataFrame 'cbchealth' loaded with 3741 rows.\n",
            "DataFrame 'nprhealth' loaded with 4837 rows.\n",
            "DataFrame 'latimeshealth' loaded with 4171 rows.\n",
            "DataFrame 'everydayhealth' loaded with 3239 rows.\n",
            "Combined DataFrame:\n",
            "                       ID                        DateTime  \\\n",
            "0      576880531301801984  Sat Mar 14 23:00:11 +0000 2015   \n",
            "1      576820122666471424  Sat Mar 14 19:00:08 +0000 2015   \n",
            "2      576744652717461504  Sat Mar 14 14:00:15 +0000 2015   \n",
            "3      576736754436304896  Sat Mar 14 13:28:52 +0000 2015   \n",
            "4      576736614766010368  Sat Mar 14 13:28:18 +0000 2015   \n",
            "...                   ...                             ...   \n",
            "20044  289382400222441472  Thu Jan 10 14:45:15 +0000 2013   \n",
            "20045  289374853075718145  Thu Jan 10 14:15:15 +0000 2013   \n",
            "20046  289371042584076289  Thu Jan 10 14:00:07 +0000 2013   \n",
            "20047  289367315647193089  Thu Jan 10 13:45:18 +0000 2013   \n",
            "20048  289367039930408961  Thu Jan 10 13:44:13 +0000 2013   \n",
            "\n",
            "                                                   Tweet  \n",
            "0      An abundance of online info can turn us into e...  \n",
            "1      A plant-based diet that incorporates fish may ...  \n",
            "2      It doesn't take much to damage your hearing at...  \n",
            "3      RT @CNN: Forever young? Discover this islandâ€™s...  \n",
            "4      RT @CNN: Is post-traumatic stress disorder in ...  \n",
            "...                                                  ...  \n",
            "20044  The top secrets to getting skinny @JillianMich...  \n",
            "20045  The surprising warning signs of mental illness...  \n",
            "20046      Untreatable #Gonorrhea? http://bit.ly/11j5ZCa  \n",
            "20047  We love your beauty secrets and guilty pleasur...  \n",
            "20048  Americans die sooner and are sicker than those...  \n",
            "\n",
            "[20049 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract URL for each article"
      ],
      "metadata": {
        "id": "By9OD5dtNOFo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We extract the URL from Tweet column of the dataframe which often contains the URL of the article the tweet mentions for a further read if interested by tweet viewer. For simplicity, we utilize the tweets with one link only and discard the rest"
      ],
      "metadata": {
        "id": "rwTULrdIeKIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract all URLs from article titles and links in combined_df\n",
        "combined_df['URL'] = [re.findall(r'https?://\\S+', text) for text in combined_df['Article Title and Link']]\n",
        "\n",
        "# Drop rows with multiple or no URLs\n",
        "multiple_URLs_indices = combined_df['Article Title and Link'][combined_df['URL'].apply(len) != 1].index\n",
        "combined_df.drop(multiple_URLs_indices, inplace=True)\n",
        "\n",
        "# Extract URL for articles\n",
        "combined_df['URL'] = [url[0] for url in combined_df['URL']]\n"
      ],
      "metadata": {
        "id": "2uO-qr0QN4uU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Article Content Extraction Functions"
      ],
      "metadata": {
        "id": "xDkojMfTNXYX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are the functions that contain the code to extract website name from URL, fetch article's HTML content from a URL, extract article's body content from HTML using BeautifulSoup and apply above functions to extract content along with storing it in a new column"
      ],
      "metadata": {
        "id": "dP6iUOXWfekX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to fetch content from a URL\n",
        "def fetch_content(url):\n",
        "    # Create a UserAgent instance\n",
        "    ua = UserAgent()\n",
        "    try:\n",
        "        # Define user-agent header using a random user agent\n",
        "        headers = {'User-Agent': ua.random}\n",
        "\n",
        "        # Create a session to handle cookies and maintain connection state\n",
        "        with requests.Session() as session:\n",
        "            response = session.get(url, headers=headers, timeout=4, allow_redirects=True)\n",
        "            response.raise_for_status()  # Raise an error for 4xx and 5xx status codes\n",
        "            final_url = response.url  # Get the final URL after following redirects\n",
        "            website_name = get_website_name(final_url)\n",
        "            return (response.text, website_name)\n",
        "    except Exception as e:\n",
        "        print(\"Error fetching content:\", e)\n",
        "        return (None, None)\n",
        "\n",
        "# Function to extract website name from URL\n",
        "def get_website_name(url):\n",
        "    parsed_url = urlparse(url)\n",
        "    return parsed_url.netloc\n",
        "\n",
        "# Function to extract body content from HTML using BeautifulSoup\n",
        "def extract_body(html_content_with_website_name):\n",
        "    html_content, website_name = html_content_with_website_name\n",
        "    if html_content:\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "        if soup:\n",
        "            if website_name == 'www.cnn.com':\n",
        "                article_body = soup.findAll('p', class_='paragraph')\n",
        "            elif website_name == 'www.health.usnews.com':\n",
        "                article_body = soup.findAll('p')\n",
        "            elif website_name == 'www.latimes.com':\n",
        "                article_body = soup.findAll('p')\n",
        "            elif website_name == 'www.npr.org':\n",
        "                storytext_div = soup.find('div', id='storytext')\n",
        "                if storytext_div:\n",
        "                    article_body = storytext_div.find_all('p')\n",
        "                else:\n",
        "                    article_body = None\n",
        "            elif website_name == 'www.cbc.ca':\n",
        "                storytext_div = soup.find('div', class_='story')\n",
        "                if storytext_div:\n",
        "                    article_body = storytext_div.find_all('p')\n",
        "                else:\n",
        "                    article_body = None\n",
        "            else:\n",
        "                article_body = None\n",
        "\n",
        "            if article_body:\n",
        "                return article_body\n",
        "            else:\n",
        "                return None\n",
        "        else:\n",
        "            return None\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Apply web scraping to extract content and store it in a new column along with progress bar\n",
        "def extract_content_with_progress(url_list):\n",
        "    content_list = []\n",
        "    for url in tqdm(url_list, desc='Extracting content'):\n",
        "        content = extract_body(fetch_content(url))\n",
        "        content_list.append(content)\n",
        "    return content_list"
      ],
      "metadata": {
        "id": "FRU39yS1Vosk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract Article Content Using URLs"
      ],
      "metadata": {
        "id": "L4pGtVGiNcWo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset is broken into 10 splits to accomodate the RAM space required and article content extraction functions are applied"
      ],
      "metadata": {
        "id": "3MCGfUagjsd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the total number of rows\n",
        "total_rows = len(combined_df)\n",
        "\n",
        "# Calculate the size of each split\n",
        "split_size = total_rows // 10\n",
        "\n",
        "# Define the start and end indices for each split\n",
        "split_1 = combined_df.iloc[:split_size].copy()\n",
        "split_2 = combined_df.iloc[split_size:2*split_size].copy()\n",
        "split_3 = combined_df.iloc[2*split_size:3*split_size].copy()\n",
        "split_4 = combined_df.iloc[3*split_size:4*split_size].copy()\n",
        "split_5 = combined_df.iloc[4*split_size:5*split_size].copy()\n",
        "split_6 = combined_df.iloc[5*split_size:6*split_size].copy()\n",
        "split_7 = combined_df.iloc[6*split_size:7*split_size].copy()\n",
        "split_8 = combined_df.iloc[7*split_size:8*split_size].copy()\n",
        "split_9 = combined_df.iloc[8*split_size:9*split_size].copy()\n",
        "split_10 = combined_df.iloc[9*split_size:].copy()"
      ],
      "metadata": {
        "id": "6H5kM__OYobU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_1['Content'] = extract_content_with_progress(split_1['URL'])\n",
        "split_1.to_csv('split_1_preprocessed.csv', index=False)"
      ],
      "metadata": {
        "id": "7oM_CgS2aJZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_2['Content'] = extract_content_with_progress(split_2['URL'])\n",
        "split_2.to_csv('split_2_preprocessed.csv', index=False)"
      ],
      "metadata": {
        "id": "Eom3GqaGwuHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_3['Content'] = extract_content_with_progress(split_3['URL'])\n",
        "split_3.to_csv('split_3_preprocessed.csv', index=False)"
      ],
      "metadata": {
        "id": "QoyyOud3cjju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_4['Content'] = extract_content_with_progress(split_4['URL'])\n",
        "split_4.to_csv('split_4_preprocessed.csv', index=False)"
      ],
      "metadata": {
        "id": "0gmAn27OwzAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_5['Content'] = extract_content_with_progress(split_5['URL'])\n",
        "split_5.to_csv('split_5_preprocessed.csv', index=False, escapechar='\\\\')"
      ],
      "metadata": {
        "id": "NgVUlb80w0zP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_6['Content'] = extract_content_with_progress(split_6['URL'])\n",
        "split_6.to_csv('split_6_preprocessed.csv', index=False, escapechar='\\\\')"
      ],
      "metadata": {
        "id": "5nLHIfjjfl82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_7['Content'] = extract_content_with_progress(split_7['URL'])\n",
        "split_7.to_csv('split_7_preprocessed.csv', index=False, escapechar='\\\\')"
      ],
      "metadata": {
        "id": "wIUiWqDEfl1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_8['Content'] = extract_content_with_progress(split_8['URL'])\n",
        "split_8.to_csv('split_8_preprocessed.csv', index=False)"
      ],
      "metadata": {
        "id": "eLTzn5iHflfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_9['Content'] = extract_content_with_progress(split_9['URL'])\n",
        "split_9.to_csv('split_9_preprocessed.csv', index=False)"
      ],
      "metadata": {
        "id": "6svbcjPlflcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_10['Content'] = extract_content_with_progress(split_10['URL'])\n",
        "split_10.to_csv('split_10_preprocessed.csv', index=False)"
      ],
      "metadata": {
        "id": "z76moJfwflX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Preprocessing"
      ],
      "metadata": {
        "id": "1SEbv2SwGjI2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading and Combining Data"
      ],
      "metadata": {
        "id": "LMVm7Qajc7eO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combining all the results of splits from previous step"
      ],
      "metadata": {
        "id": "828Guc3FkLrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List to store DataFrames of all splits\n",
        "dfs = []\n",
        "\n",
        "# Read each CSV file and append its DataFrame to the list\n",
        "for i in range(1, 11):\n",
        "    filename = f'/content/split_{i}_preprocessed.csv'\n",
        "    df = pd.read_csv(filename)\n",
        "    dfs.append(df)\n",
        "\n",
        "# Concatenate all DataFrames in the list\n",
        "combined_df = pd.concat(dfs, ignore_index=True)"
      ],
      "metadata": {
        "id": "eftyxjYUnAUC"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Format and Clean Data"
      ],
      "metadata": {
        "id": "52_HOn5vdhDo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Formatting string to list, concatenating strings within list, cleaning text, removing 'nan' and NaNs from the text"
      ],
      "metadata": {
        "id": "EBr0FWr0mdnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all the content to list type\n",
        "combined_df['Content'] = combined_df['Content'].apply(lambda x: [x])\n",
        "\n",
        "# Combine content paragraphs into a single string\n",
        "combined_df['Content'] = [\" \".join([str(p_tag).strip() for p_tag in content]) for content in combined_df['Content']]"
      ],
      "metadata": {
        "id": "58xoEhRuOjL5"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to clean text\n",
        "def clean_text(text):\n",
        "    # Remove HTML tags\n",
        "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
        "    # Substitute hyphens with empty spaces\n",
        "    text = re.sub(r'-', ' ', text)\n",
        "    # Remove non-alphabetic characters\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "# Apply the clean_text function to the 'Content' column\n",
        "combined_df['Content'] = combined_df['Content'].apply(clean_text)"
      ],
      "metadata": {
        "id": "6u_IwlTNOjL6"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove 'nan' strings\n",
        "combined_df['Content'] = combined_df['Content'].replace('nan', np.nan)\n",
        "\n",
        "# Drop rows with missing content\n",
        "combined_df.dropna(subset=['Content'], inplace=True)"
      ],
      "metadata": {
        "id": "X2ndW64D8aLH"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing Data"
      ],
      "metadata": {
        "id": "BSagouBSdmpY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here as part of preprocessing, we do word tokenization, pos tagging, lemmatization and stop word removal on the text"
      ],
      "metadata": {
        "id": "b2i-Q_fGrO3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to wordnet tags\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return None  # Use default POS for lemmatization"
      ],
      "metadata": {
        "id": "oGZM5CJvOjL6"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and preprocess text data\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text)  # Tokenize text\n",
        "    lemmatizer = WordNetLemmatizer()  # Initialize lemmatizer\n",
        "    pos_tags = nltk.pos_tag(tokens)  # Get part-of-speech tags\n",
        "    for i, (token, tag) in enumerate(pos_tags):\n",
        "        pos = get_wordnet_pos(tag)  # Convert NLTK POS tags to WordNet POS tags\n",
        "        if pos:\n",
        "            tokens[i] = lemmatizer.lemmatize(token, pos=pos)  # Lemmatize tokens\n",
        "        else:\n",
        "            tokens[i] = lemmatizer.lemmatize(token)  # Use default POS for lemmatization\n",
        "    stop_words = set(stopwords.words('english'))  # Get stopwords\n",
        "    custom_stopwords = [    # Custom stopwords\n",
        "    \"patient\", \"doctor\", \"say\", \"year\", \"state\", \"day\", \"need\", \"come\", \"well\",\n",
        "    \"make\", \"think\", \"know\", \"go\", \"use\", \"one\", \"like\", \"people\", \"may\",\n",
        "    \"many\", \"still\", \"even\", \"two\", \"way\", \"good\", \"much\", \"back\", \"new\",\n",
        "    \"time\", \"first\", \"really\",\n",
        "    \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\",\n",
        "    \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\"\n",
        "    ]\n",
        "    tokens = [token for token in tokens if token not in stop_words and token not in custom_stopwords]  # Remove stopwords\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "combined_df['Content'] = combined_df['Content'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "0K2VTcFjOjL7"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving Data for Modeling"
      ],
      "metadata": {
        "id": "j5Nv94LkdvsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save preprocessed data to CSV\n",
        "combined_df.to_csv('combined_preprocessed.csv', index=False)"
      ],
      "metadata": {
        "id": "PbXfpmHKOjL7"
      },
      "execution_count": 35,
      "outputs": []
    }
  ]
}